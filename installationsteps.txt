Step 1
Download helm
Go to
https://github.com/helm/helm/releases
download the file 
Windows amd64 (hyperlink)

#set env variable

#Check helm version

helm version

Step2
#prometheus installation
#check helm repository list

helm repo list

#Add repository
helm repo add bitnami https://charts.bitnami.com/bitnami

#prometheus installation

Step 1: Add the Prometheus Helm Chart Repository

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

Update your Helm repository:
helm repo update
Step 2:
kubectl create namespace prometheus

Step3:

helm install prometheus prometheus-community/prometheus -n prometheus

Step 4:

helm list -n prometheus
Step5:

kubectl get pods -n prometheus

#if node exporter pod fails, run the patch
kubectl patch -n prometheus ds prometheus-prometheus-node-exporter --type "json" -p "[{"op": "remove", "path" : "/spec/template/spec/containers/0/volumeMounts/2/mountPropagation"}]"

Step6
#Run the prometheus server, push gateway , node exporter and alert manager in localhost
#follow the steps below
#open bash script in administrator mode
#go to the folder where we have port-forward.sh
chmod 777 ./port-forward.sh
./port-forward.sh

Step 7
Go to browser
#prometheus server
http://localhost:9090
#push gateway 
http://localhost:9091
#node exporter
http://localhost:9100
#alert manager
http://localhost:9093

#grafana installation
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
kubectl create namespace grafana
helm install my-grafana grafana/grafana -n grafana
kubectl get pods,svc -n grafana
#expose grafana as service in localhost
kubectl expose deployment --port 3000 my-grafana -n grafana --type=LoadBalancer --name=grafanaservice
#use bash script
kubectl get secret --namespace grafana my-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
#open browser to access grafana
http://localhost:3000
#modify grafana
helm show values grafana/grafana > values.yaml
helm upgrade my-grafana grafana/grafana -f values.yaml -n grafana
#telegraf installation
helm repo add influxdata https://helm.influxdata.com
helm repo update
#install influxdb
kubectl create ns influxdbns
helm install influxdb influxdata/influxdb2 --set adminuser.enabled=true --set adminuser.user=admin --set adminuser.password=admin123 --set adminuser.org=rps --set adminuser.bucket=training --set adminuser.token=mysecret -n influxdbns
#expose to port no 8086
kubectl port-forward -n influxdbns svc/influxdb-influxdb2 8086:80
#get the password using bashscript
kubectl get secret influxdb-influxdb2-auth -o "jsonpath={.data['admin-password']}" --namespace influxdbns | base64 --decode
#access db from browser
http://localhost:8086

#to find out dns name
kubectl exec -it <podname> -n <namespace> -- /bin/sh
#nslookup <influx service name>
#exit

#health check
kubectl run -i --tty dns-test --image=busybox --restart=Never -- sh
wget -qO- http://influxdb-influxdb2.default.svc.cluster.local:80/health
#result should be 
{"name":"influxdb", "message":"ready for queries and writes", "status":"pass", "checks":[], "version": "v2.7.4", "commit": "19e5c0e1b7"}

#how to get influxdb configuration values
helm list -n <namespace>
helm get values influxdb -n <namespace>

#api token


kubectl create ns telegraf
helm install telegraf influxdata/telegraf -n telegraf



#create the new folder
helm show values influxdata/telegraf > values.yaml


#add output as Prometheus client
outputs:
    - influxdb_v2:
        urls: ["http://influxdb-influxdb2.default.svc.cluster.local:80"]  #use influx db dns name
        token: "g43keiCtnRMWYLR0SFIEZwgwEq2hW4-i8FwS7ksplhP7G2pGgJ3Ej-QdyQepxysw4S42mLNOa_xcdhmMc-YV_g=="
        organization: "rps"
        bucket: "training"
    - prometheus_client:
        listen: ":9273"
        path: "/metrics" 
  inputs:
    - statsd:
        service_address: ":8125"
        percentiles:
          - 50
          - 95
          - 99
        metric_separator: "_"
        allowed_pending_messages: 10000
        percentile_limit: 1000
    - cpu:
        percpu: true
        totalcpu: true
        collect_cpu_time: false
        report_active: false

    - mem: {}

    - disk:
        ignore_fs:
          - tmpfs
          - devtmpfs
          - devfs

    - diskio: {}

    - system: {}

helm upgrade telegraf -n telegraf influxdata/telegraf  -f values.yaml
kubectl port-forward svc/telegraf -n telegraf 9273:9273
http://localhost:9273/metrics

#integrate telegraf and prometheus server add telegraf as target to prometheus
#open new folder for prom configuration

helm show values prometheus-community/prometheus > values.yaml

#Add the telegraf job to prometheus.yml inside values.yaml
- job_name: 'telegraf'
        metrics_path: '/metrics'
        static_configs:
          - targets: ['host.docker.internal:9273']

helm upgrade prometheus prometheus-community/prometheus -f values.yaml -n prometheus
#creating open telemetry
helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
helm repo update
helm install my-otel-collector open-telemetry/opentelemetry-collector --set image.repository=otel/opentelemetry-collector-contrib --set image.tag=latest --set mode=deployment
#create otlp foloder and download
helm show values open-telemetry/opentelemetry-collector > values.yaml

modify values.yaml
config:
  exporters:
    debug: {}
    prometheus:
      endpoint: "0.0.0.0:8889"
  extensions:
    # The health_check extension is mandatory for this chart.
    # Without the health_check extension the collector will fail the readiness and liveliness probes.
    # The health_check extension can be modified, but should never be removed.
    health_check:
      endpoint: 0.0.0.0:13133
  processors:
    batch: {}
    # Default memory limiter configuration for the collector based on k8s resource limits.
    memory_limiter:
      # check_interval is the time between measurements of memory usage.
      check_interval: 5s
      # By default limit_mib is set to 80% of ".Values.resources.limits.memory"
      limit_percentage: 80
      # By default spike_limit_mib is set to 25% of ".Values.resources.limits.memory"
      spike_limit_percentage: 25
  receivers:
    jaeger:
      protocols:
        grpc:
          endpoint: 0.0.0.0:14250
        thrift_http:
          endpoint: 0.0.0.0:14268
        thrift_compact:
          endpoint: 0.0.0.0:6831
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
    # if internalTelemetryViaOTLP.metrics.enabled = true, prometheus receiver will be removed
    prometheus:
      config:
        scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
              - targets:
                  - 0.0.0.0:8889
    zipkin:
      endpoint: 0.0.0.0:9411

service:
    telemetry:
      metrics:
        readers:
          - pull:
              exporter:
                prometheus:
                  host: 0.0.0.0
                  port: 8888
    extensions:
      - health_check
    pipelines:
      logs:
        exporters:
          - debug
        processors:
          - memory_limiter
          - batch
        receivers:
          - otlp
      metrics:
        exporters: [debug,prometheus]
          #- debug
        processors:
          - memory_limiter
          - batch
        receivers:
          - otlp
          # if internalTelemetryViaOTLP.metrics.enabled = true, prometheus receiver will be removed
          - prometheus
      traces:
        exporters:
          - debug
        processors:
          - memory_limiter
          - batch
        receivers:
          - otlp
          - jaeger
          - zipkin
ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    hostPort: 4317
    protocol: TCP
    # nodePort: 30317
    appProtocol: grpc
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    hostPort: 4318
    protocol: TCP
  jaeger-compact:
    enabled: true
    containerPort: 6831
    servicePort: 6831
    hostPort: 6831
    protocol: UDP
  jaeger-thrift:
    enabled: true
    containerPort: 14268
    servicePort: 14268
    hostPort: 14268
    protocol: TCP
  jaeger-grpc:
    enabled: true
    containerPort: 14250
    servicePort: 14250
    hostPort: 14250
    protocol: TCP
  zipkin:
    enabled: true
    containerPort: 9411
    servicePort: 9411
    hostPort: 9411
    protocol: TCP
  metrics:
    # The metrics port is disabled by default. However you need to enable the port
    # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
    enabled: true
    containerPort: 8889
    servicePort: 8889
    protocol: TCP


helm upgrade my-otel-collector -f values.yaml open-telemetry/opentelemetry-collector --set image.repository=otel/opentelemetry-collector-contrib --set image.tag=latest --set mode=deployment
kubectl port-forward svc/my-otel-collector-opentelemetry-collector 4317:4317 4318:4318 8889:8889

#install jaeger for call tracing
#download jaegerkube.zip file and extract it
#go to folder level
kubectl apply -f jaeger-all-in-one.yaml
kubectl get pods
kubectl get svc jaeger
kubectl port-forward service/jaeger 16686:16686
http://localhost:16686

#install mimir and minio
#download mimir values.yaml and store it in mimir configuration folder
helm install mimir grafana/mimir-distributed -f values.yaml
#use bash script
kubectl get secret mimir-minio -o jsonpath="{.data.rootUser}" | base64 --decode
grafana-user
kubectl get secret mimir-minio -o jsonpath="{.data.rootPassword}" | base64 --decode
supersecret
#use command prompt
kubectl port-forward svc/mimir-minio-console 9001:9001
kubectl port-forward svc/mimir-minio 9000:9000

kubectl port-forward svc/mimir-distributed-nginx 9009:80
curl http://localhost:9009/prometheus/api/v1/status/buildinfo
kubectl port-forward svc/mimir-distributed-query-frontend 9010:8080
helm get values mimir-distributed


